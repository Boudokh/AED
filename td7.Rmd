---
title: 'TD 7: Multiple Linear Regression'
author: "Université de Paris"
Students : "Inès Dardouri , Lycia Fezzoua et Mohamed Boudokhane"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
subtitle: Exploratory Data Analysis M1 MIDS
---

Multiple Linear Regression
==========================

- Reading linear models summaries
- Analysis of Variance
- Variable selection problem
- Forward-Backward methods
- Penalization



## Simulated data: Gaussian Linear modeling


```{r, echo=FALSE, message=FALSE, warning=FALSE}
require(tidyverse)
```

```{r}
set.seed(1515)  # pick your student number instead of 1515

n <- 1000
p <- 10
B <- 2000
s <- 5

sigma <- 1
```

### Regular grid, polynomials

Use function `poly()` to create a design matrix with `n` rows and `p` columns.
The $i^{\text{th}}$ row is made of $(x_i^k)_{k=0, \ldots, p-1}$ with $x_i= \frac{i}{n+1}$.

Call the design $Z$, name the columns `x0, x1, ...`.

```{r}
grid <- (1:n)/(n+1)
Z <- cbind(rep(1,n), poly(grid, degree=p-1, raw = TRUE))
Z <- as.data.frame(Z)
colnames(Z) <- stringr::str_c("x", 0:(p-1), sep = "")
```

### Design properties

- Compute the design SVD. 

- Plot the singular values

- Compute the QR decomposition of the design matrix

- Compute the _pseudo-inverse_ of the design matrix 

```{r}
# TODO

svd.Z = svd(Z)
is.list(svd.Z)
names(svd.Z)
sv = svd.Z$d
#ou svd.Z[['d']]
qplot(x=seq_along(sv), y=sv, geom="col",xlab ="rank", ylab="singular values")+
  scale_y_log10() +
  ggtitle("Poorly conditionned design")

```


```{r}
pacman::p_load(corpcor)
Z_QR = qr(Z)
#qr.Q(Z_QR)
pseudoinverse(as.matrix(Z))
#piv(Z)
qr.solve(Z_QR,Y)


```

### Linear models with fixed designrm()

Generate a random polynomial of degree `p-1` with `s` non-null coefficients 
$1, 2, \ldots, s$. The position of the non-null coefficients should be random. Their 
value 


```{r}
ix <- sort(sample(1:p, s))
betas <- rep(0, p)
betas[ix] <- 1:s

betas
```

Generate an instance of the linear model: 
\[\begin{bmatrix}
y_1 \\
\vdots\\
y_n
\end{bmatrix}
= \underbrace{\begin{bmatrix}
1 & x^1_1 & \ldots & x_1^{p-1} \\
\vdots &  & & \vdots \\
1 & x^1_n & \ldots & x_n^{p-1} \\
\end{bmatrix}}_{Z}
\times \begin{bmatrix} \beta_0\\
\vdots \\
\beta^{p-1}
\end{bmatrix}
+ \sigma \begin{bmatrix}
\epsilon_1 \\
\vdots \\
\epsilon_n
\end{bmatrix}\]

```{r}
# TODO:
Y = as.matrix(Z) %*% betas + sigma * rnorm(n)
df <- cbind(Z,Y)
assertthat::assert_that(all(names(df) == c("x0", "x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9", "Y")))
```

Compute the least squares estimate of $\beta$ given $Z$ and $Y$.
Use `qr.solve()` and `lm()`. Compare the results. 

```{r}
# TODO:
betas_hat = qr.solve(Z_QR,Y)
betas_hat
lm.0 = lm(formula = Y ~ . -1,df)
lm.0

norm(coefficients(lm.0) - betas_hat)
```

Comment the output of `summary(.)`

```{r}
# TODO
summary(lm.0)

# We want our t-value coefficient to be far away from zero as this would indicate we could reject the null hypothesis. In our example, the t-statistic values are relatively close to zero. So, we cannot reject the null hypothesis.

#A small p-value indicates that it is unlikely we will observe a relationship between the predictor and response variables due to chance. Typically, a p-value of 5% or less is a good cut-off point. In our example, the p-values are not very close to zero. Consequently, we cannot reject the null hypothesis for x1 to x9.

```

- Provide an interpretation of each $t$ value 
- Provide an interpretation of `Pr(>|t|)`
- Explain why the `Residual standard error` has `990` degrees of freedom. 
- How is `Residual standard error` computed? 
- What is `Residual standard error` supposed to estimate?
- Is it an unbiased estimator? 
- What is `Multiple R-squared` ?
- What is `Adjusted R-squared` ?
- How do you reconcile the values of `Pr(>|t|)`  and the `p-value` of the `F-statistic`

Have a look at the body of `summary` method for class `lm` (`summary.lm`). 

```{r}
# TODO:
summary.lm

```

Plot $Y$ and fitted values $\widehat{Y}$ against $x$ (second column of design, `x^1`).  

```{r}
# TODO:

df %>%
  ggplot(aes(x=x1,y=Y))+
  geom_point(size=.5, alpha=.3)+
  geom_line(mapping=aes(y=fitted(lm.0)),col="red")+
  geom_line(mapping = aes(y=as.matrix(Z)%*%betas))+
  ggtitle("Multilinear regression with full model (polynomial with degree 9)")

#fitted = vals attendues (y^)

```


## Analysis of variance 

Fit the minimal model (polynomial of degree $0$)
to your observations. Comment summary.  

```{r}
# TODO: 
lm00 <- lm(Y ~ -1, df)
```

Compare minimal model and full model with function `anova`. Comment.  

```{r}
# TODO: 
anova(lm00,lm.0)
```

## Variable selection 

From now on we sall use package `MASS` to perform variable selection using function `stepAIC`.

```{r}
require(MASS)
```

We pick another model with the same design matrix, but the estimand $\beta$
shall be `r c(5:1, rep(0, 5))`

```{r}
betas <- c(5:1, rep(0, 5))

noise = rnorm(n)
Y = as.matrix(Z) %*% betas + sigma*noise
df2 <- cbind(Z,Y)
```

Fit polynomials of degree `0` up to `9` to the new data

You may define the formulas in different ways. This is pedestrian. You may also 
use `poly()`. 

```{r}
formulas <- purrr::map_chr(0:9 , .f = ~ stringr::str_c("x", 0:., collapse = " + ")) %>% 
  stringr::str_c("Y ~ ", .) %>% 
  stringr::str_c( " - 1") %>% 
  purrr::map(formula)

lms = formulas %>% purrr::map(lm,data=df2)
```

Plot  sums of squared residuals as a function of order.

```{r}
# TODO:

df %>%
  ggplot(aes(x=x0*x0,y=Y),xlab ="sums of squared residuals", ylab="order")+
  geom_point(size=.5, alpha=.3)+
  geom_line(mapping=aes(y=fitted(lm.0)),col="red")+
  
  ggtitle("Sums of squared residuals as a function of order.")

```


Perform `anova` analysis between polynomial model of degree `i`  and polynomial model
of degree `i + 1` for `0 <= i <9`. 

```{r}
# TODO: 
anova(lms[[1]], lms[[2]])
```


```{r}
# TODO:
for (i in 1:9){
  print(anova(lms[[i]], lms[[i+1]]))
}
  
```

Use function `aov()`,  comment result and summary.

```{r}
# TODO:
df2.aov <- aov(formulas[[10]], data = df2)

summary(df2.aov)

# @Mo, ici il faut faire aov(Y~xi) pour chaque xi qu'on a décidé de garder pcq il est utile
# c'est lesquels qu'il faut garder je me rappelle plus comment on fait mais tu m'avais
# expliqué ça au td6

#♠une fois qu'on sait ça, on peut faire le reste avec <3
```

Update `df2.aov` using function `update`. Drop `x5, x6, x7, x8, x9`

```{r}
# TODO: 
update(df2.aov, Y ~ix - x5 - x6 - x7 - x8 - x9)
```

Use function `dropterm` on full model with option `test = "F"` and without. 
Interpret in both cases. 

```{r}
# TODO: 
dropterm(df2.aov, scale = 0, test = "F")
dropterm(df2.aov, scale = 0,k=2)
```

## Using `addterm` between minimal and full model


```{r}
# TODO:
addterm(df2, upper = ~ ., comp = 1:9,
        aic = TRUE)

dropterm(object, lower = ~ 1, comp = 1:9,
         aic = TRUE, bestmodel = .)
```

## Using `stepAIC()`

Use function `stepAIC()` to search for a good balance 
between model complexity and goodness of fit. 

Comment. 

```{r}
# TODO

stepAIC(df2.aov, scope = list(upper=formulas[[10]], lower=formulas[[1]]))

```

