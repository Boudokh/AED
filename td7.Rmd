---
title: 'TD 7: Multiple Linear Regression'
author: "Université de Paris"
Students : "Inès Dardouri , Lycia Fezzoua et Mohamed Boudokhane"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
subtitle: Exploratory Data Analysis M1 MIDS
---

Multiple Linear Regression
==========================

- Reading linear models summaries
- Analysis of Variance
- Variable selection problem
- Forward-Backward methods
- Penalization



## Simulated data: Gaussian Linear modeling


```{r, echo=FALSE, message=FALSE, warning=FALSE}
require(tidyverse)
```

```{r}
set.seed(21504397)  # pick your student number instead of 1515

n <- 1000
p <- 10
B <- 2000
s <- 5

sigma <- 1
```

### Regular grid, polynomials

Use function `poly()` to create a design matrix with `n` rows and `p` columns.
The $i^{\text{th}}$ row is made of $(x_i^k)_{k=0, \ldots, p-1}$ with $x_i= \frac{i}{n+1}$.

Call the design $Z$, name the columns `x0, x1, ...`.

```{r}
grid <- (1:n)/(n+1)
Z <- cbind(rep(1,n), poly(grid, degree=p-1, raw = TRUE))
Z <- as.data.frame(Z)
colnames(Z) <- stringr::str_c("x", 0:(p-1), sep = "")
```

### Design properties

- Compute the design SVD. 

- Plot the singular values

- Compute the QR decomposition of the design matrix

- Compute the _pseudo-inverse_ of the design matrix 

```{r}
# TODO

svd.Z = svd(Z)
is.list(svd.Z)
names(svd.Z)
sv = svd.Z$d
#ou svd.Z[['d']]
qplot(x=seq_along(sv), y=sv, geom="col",xlab ="rank", ylab="singular values")+
  scale_y_log10() +
  ggtitle("Poorly conditionned design")

```


```{r}
#QR decomposition :

Z_QR = qr(Z)
Q <- qr.Q(Z_QR)
R <- qr.R(Z_QR)
#qr.solve(Z_QR,Y)
#Q

#pseudo-inverse :
pseudoinverse(as.matrix(Z))



```

### Linear models with fixed designrm()

Generate a random polynomial of degree `p-1` with `s` non-null coefficients 
$1, 2, \ldots, s$. The position of the non-null coefficients should be random. Their 
value 


```{r}
ix <- sort(sample(1:p, s))
betas <- rep(0, p)
betas[ix] <- 1:s

betas
```

Generate an instance of the linear model: 
\[\begin{bmatrix}
y_1 \\
\vdots\\
y_n
\end{bmatrix}
= \underbrace{\begin{bmatrix}
1 & x^1_1 & \ldots & x_1^{p-1} \\
\vdots &  & & \vdots \\
1 & x^1_n & \ldots & x_n^{p-1} \\
\end{bmatrix}}_{Z}
\times \begin{bmatrix} \beta_0\\
\vdots \\
\beta^{p-1}
\end{bmatrix}
+ \sigma \begin{bmatrix}
\epsilon_1 \\
\vdots \\
\epsilon_n
\end{bmatrix}\]

```{r}
# TODO:
Y = as.matrix(Z) %*% betas + sigma * rnorm(n)
df <- cbind(Z,Y)
assertthat::assert_that(all(names(df) == c("x0", "x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9", "Y")))
```

Compute the least squares estimate of $\beta$ given $Z$ and $Y$.
Use `qr.solve()` and `lm()`. Compare the results. 

```{r}
# TODO:
betas_hat = qr.solve(Z_QR,Y)
betas_hat
lm.0 = lm(formula = Y ~ . -1,df)
lm.0

norm(coefficients(lm.0) - betas_hat)
```

Comment the output of `summary(.)`

```{r}
# TODO
summary(lm.0)


```

- Provide an interpretation of each $t$ value 
The coefficient t-value is a measure of how many standard deviations our coefficient estimate is far away from 0.
The value $|t|$ is high when the `Std. Error` is low. So the bigger the value $|t|$ is the bigger the correlation between $x$ and $y$.
We want our t-value coefficient to be far away from zero as this would indicate we could reject the null hypothesis. In our example, the t-statistic values are relatively close to zero. So, we cannot reject the null hypothesis.

- Provide an interpretation of `Pr(>|t|)`

The Pr(>t) acronym found in the model output relates to the probability of observing any value equal or larger than t. A small p-value indicates that it is unlikely we will observe a relationship between the predictor and response variables due to chance.
In our example, the p-values are not very close to zero. Consequently, we cannot reject the null hypothesis for x1 to x9.

- Explain why the `Residual standard error` has `990` degrees of freedom. 

We have a sample of 1000 observations and 10 variables so 1000 - 10 = 990 = degrees of freedom.
The coefficient Standard Error measures the average amount that the coefficient estimates vary from the actual average value of our response variable, which means that 10 residuals can be deduced if we have the other 990.

- How is `Residual standard error` computed? .
With `SSR`, the sum of squared residuals, and `d.f` = 990 the degree of freedom, we have :

`Residual standard error` $= \sqrt{\frac{SSR}{d.f}}$

- What is `Residual standard error` supposed to estimate?

The `Residual standard error` measures the average amount that the coefficient estimates vary from the actual average value of our response variable. We’d ideally want a lower number relative to its coefficients.
(écart-type)

- Is it an unbiased estimator?  Yes, it is.

- What is `Multiple R-squared` ?

SSR = the sum of the squared residuals
SST = the total sum of the squares 

`Multiple R-squared` $= 1 - \frac{SSR}{SST}$

This quantity is a statistic. If it is close to 1, we can say that the model is adequate to the observations.  

- What is `Adjusted R-squared` ?

The Adjusted R-squared includes a term that penalizes a model for each additional explanatory variable :
v = number of explanatory variables

`Adjusted R-squared` $= 1 - \frac{SSR}{SST}*\frac{n-1}{n-v-1}$

- How do you reconcile the values of `Pr(>|t|)`  and the `p-value` of the `F-statistic`

F-statistic is a good indicator of whether there is relationship between our predictor variables and the response variable. 
If our F-statistic is significant, we can trust on the `R-squared` value more. 

Here, our p-value is much less than 5% (<2.2e-16), so we can say that there is a significant relationship between the variables in the linear regression model of the dataset.

********

Have a look at the body of `summary` method for class `lm` (`summary.lm`). 

```{r}
# TODO:
summary.lm

```

Plot $Y$ and fitted values $\widehat{Y}$ against $x$ (second column of design, `x^1`).  

```{r}
# TODO:

df %>%
  ggplot(aes(x=x1,y=Y))+
  geom_point(size=.5, alpha=.3)+
  geom_line(mapping=aes(y=fitted(lm.0)),col="red")+
  geom_line(mapping = aes(y=as.matrix(Z)%*%betas))+
  ggtitle("Multilinear regression with full model (polynomial with degree 9)")

#fitted = vals attendues (y^)

```


## Analysis of variance 

Fit the minimal model (polynomial of degree $0$)
to your observations. Comment summary.  

```{r}
# TODO: 
lm00 <- lm(Y ~ -1, df)
lm00
summary(lm00)


```

-> Residuals are essentially the difference between the actual observed response values and the response values that the model predicted.
We should look for a symmetrical distribution across the summary points on the mean value zero. In our example, we can see that the distribution of the residuals do not appear to be strongly symmetrical. That means that the model predicts certain points that fall far away from the actual observed points. which tells us that there is a strong correlation between x0 and Y.

********

Compare minimal model and full model with function `anova`. Comment.  

```{r}
# TODO: 
anova(lm00,lm.0)

```

-> We can see that for the full model, the p-value is very close to zero (3*), so this anova table tells us that there is good reason to use the full model over the minimal model.

********

## Variable selection 

From now on we sall use package `MASS` to perform variable selection using function `stepAIC`.

```{r}
require(MASS)
```

We pick another model with the same design matrix, but the estimand $\beta$
shall be `r c(5:1, rep(0, 5))`

```{r}
betas <- c(5:1, rep(0, 5))

noise = rnorm(n)
Y = as.matrix(Z) %*% betas + sigma*noise
df2 <- cbind(Z,Y)
```

Fit polynomials of degree `0` up to `9` to the new data

You may define the formulas in different ways. This is pedestrian. You may also 
use `poly()`. 

```{r}
formulas <- purrr::map_chr(0:9 , .f = ~ stringr::str_c("x", 0:., collapse = " + ")) %>% 
  stringr::str_c("Y ~ ", .) %>% 
  stringr::str_c( " - 1") %>% 
  purrr::map(formula)

lms = formulas %>% purrr::map(lm,data=df2)
```

Plot  sums of squared residuals as a function of order.

```{r}
# TODO:

ss<-0
for (i in 1:9) {
  ss[i] <- sum(residuals(lms[[i]])^2)
}

ssr <- data.frame(order = c(1:9), values = ss)
ssr %>%
  ggplot(aes(x=order,y=values)) + xlab ("order")+ ylab("sums of squared residuals")+
  geom_point(size=1.5, alpha=.8, col="red")+
  #geom_line(mapping=aes(y=fitted(lm.0)),col="red")+
  ggtitle("Sums of squared residuals as a function of order.")



```


Perform `anova` analysis between polynomial model of degree `i`  and polynomial model
of degree `i + 1` for `0 <= i <9`. 

```{r}
# TODO: 
anova(lms[[1]], lms[[2]])
```


```{r}
# TODO:
for (i in 1:9){
  print(anova(lms[[i]], lms[[i+1]]))
}
  
```

Use function `aov()`,  comment result and summary.

```{r}
# TODO:
df2.aov <- aov(formulas[[10]], data = df2)

summary(df2.aov)


```

-> We can see that with aov, there are more significantly close to zero p-values, comparing to summary(lm.0), which are x0, x1, x2 and x3.

********

Update `df2.aov` using function `update`. Drop `x5, x6, x7, x8, x9`

```{r}
# TODO: 
df2_updated <- update(df2.aov, . ~ . - x5 - x6 - x7 - x8 - x9)
df2_updated

```

Use function `dropterm` on full model with option `test = "F"` and without. 
Interpret in both cases. 

```{r}
# TODO: 
dropterm(df2.aov, scale = 0, test = "F")
dropterm(df2.aov, scale = 0,k=2)


```

-> We clearly notice that with the option `test = "F"`, we have more information that can help us decide. Indeed, without this option, we have no information about the p-values, so we cannot know if they are close enough to zero or not.
In conclusion, we better use this function with the `test = "F"` option.


********

## Using `addterm` between minimal and full model


```{r}
# TODO:

addterm(lm00,lm.0)
```

## Using `stepAIC()`

Use function `stepAIC()` to search for a good balance 
between model complexity and goodness of fit. 

Comment. 

```{r}
# TODO

stepAIC(df2.aov, scope = list(upper=formulas[[10]], lower=formulas[[1]]))

```

-> The model with just x0, x1, x2 and x3 and without the rest of the xi, is a pretty good model. So, we can only keep those.
